{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "Logistic Regression is a supervised learning algorithm used for binary classification problems, where the target variable has two possible outcomes (e.g., 0 or 1, True or False, Spam or Not Spam). Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that a given input belongs to a particular class.\n",
    "\n",
    "#### **1. The Logistic Function (Sigmoid Function)**\n",
    "Logistic Regression uses the **sigmoid function** (also called the logistic function) to transform the linear output into a probability:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ z = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n $ (the linear combination of input features)\n",
    "- $ w_i $ are the model parameters (weights)\n",
    "- $ x_i $ are the input features\n",
    "- $ e $ is Euler's number (~2.718)\n",
    "\n",
    "The output of this function lies in the range (0,1), making it interpretable as a probability.\n",
    "\n",
    "#### **2. Decision Boundary**\n",
    "To classify an instance, we define a threshold (typically 0.5):\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } \\sigma(z) \\geq 0.5 \\\\\n",
    "0, & \\text{if } \\sigma(z) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This separates the feature space into two regions, each corresponding to a different class.\n",
    "\n",
    "### **Evaluating a Logistic Regression Model**\n",
    "\n",
    "When evaluating a logistic regression model, we use various metrics such as **accuracy, precision, recall, F1 score, ROC-AUC and the confusion matrix**. Let's go through each of these with examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Accuracy **\n",
    "**Definition**: Accuracy is the proportion of correctly predicted outcomes out of the total predictions.\n",
    "\n",
    "#### **Example Scenario**:\n",
    "Suppose we build a logistic regression model to classify whether an email is spam (1) or not spam (0). Given 100 emails:\n",
    "- 90 emails are correctly classified (either spam or not spam).\n",
    "- 10 emails are misclassified.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} = \\frac{90}{100} = 90\\%\n",
    "$$\n",
    "\n",
    "**Limitations**:\n",
    "- If the dataset is highly imbalanced (e.g., 95% non-spam, 5% spam), a model predicting \"not spam\" for every email would still achieve 95% accuracy but fail at identifying spam emails.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Precision, Recall, and F1 Score **\n",
    "These metrics are particularly useful when dealing with imbalanced data.\n",
    "\n",
    "#### **Example Scenario**:\n",
    "Consider a **medical test** to detect a disease (1 = Disease, 0 = No Disease). The model produces the following results:\n",
    "\n",
    "- **True Positives (TP)** = 40 (actual diseased patients correctly identified)\n",
    "- **False Positives (FP)** = 10 (healthy people incorrectly classified as diseased)\n",
    "- **False Negatives (FN)** = 30 (diseased people incorrectly classified as healthy)\n",
    "\n",
    "##### **Precision Calculation**\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}} = \\frac{40}{40 + 10} = \\frac{40}{50} = 80\\%\n",
    "$$\n",
    "**Interpretation**: When the model predicts \"disease,\" it is correct 80% of the time.\n",
    "\n",
    "##### **Recall (Sensitivity) Calculation**\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} = \\frac{40}{40 + 30} = \\frac{40}{70} = 57.1\\%\n",
    "$$\n",
    "**Interpretation**: The model correctly identifies 57.1% of all diseased patients.\n",
    "\n",
    "### **2. Confusion Matrix **\n",
    "The confusion matrix provides a breakdown of predictions.\n",
    "\n",
    "#### **Example Scenario**:\n",
    "A bank uses a logistic regression model to predict whether a customer will default on a loan.\n",
    "\n",
    "| Actual \\ Predicted | No Default (0) | Default (1) |\n",
    "|-------------------|--------------|-------------|\n",
    "| **No Default (0)**  | 50 (TN)  | 5 (FP)  |\n",
    "| **Default (1)**  | 10 (FN)  | 35 (TP)  |\n",
    "\n",
    "- **True Negatives (TN) = 50** (Correctly predicted no default)\n",
    "- **False Positives (FP) = 5** (Wrongly predicted default for a customer who didnâ€™t default)\n",
    "- **False Negatives (FN) = 10** (Wrongly predicted no default for a customer who actually defaulted)\n",
    "- **True Positives (TP) = 35** (Correctly predicted default)\n",
    "\n",
    "**Insights**:\n",
    "- A high **false negative rate** (FN = 10) could mean the bank is **underestimating loan risk**, leading to financial loss.\n",
    "- A high **false positive rate** (FP = 5) means some customers were wrongly denied loans.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. F1 Score Calculation**\n",
    "\n",
    "F1 Score is the harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{(0.80) \\times (0.571)}{0.80 + 0.571} = 66.6\\%\n",
    "$$\n",
    "\n",
    "**Interpretation**: F1 score balances precision and recall. If missing a diseased patient is costly, recall should be prioritized over precision.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **4. ROC Curve & AUC Example**\n",
    "**Definition**: The Receiver Operating Characteristic (ROC) curve plots **True Positive Rate (Recall) vs. False Positive Rate (FPR)**. The **Area Under Curve (AUC)** measures model performance (higher AUC = better model).\n",
    "\n",
    "#### **Example Scenario**:\n",
    "Suppose we have two loan default prediction models:\n",
    "- **Model A**: AUC = 0.85 (85% chance of correctly ranking a default case higher than a non-default case)\n",
    "- **Model B**: AUC = 0.65 (65% chance)\n",
    "\n",
    "Since **Model A has a higher AUC (0.85 vs. 0.65), it is better at distinguishing defaults from non-defaults**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
